# knn部分
## 主要是复习一下knn原理，还有向量化代码的编写。
## 2-loop的代码很朴素，很容易想；1-loop的代码的关键点应该在于那个广播？(但为什么反而1-loop的运行时间还比2-loop的运行时间要长？)     no-loop则不是简单的广播。如果还是沿着之前的思路，把X扩展为X[np.newaxis, :, :]，把self.X_train扩展为self.X_train[:, np.newaxis, :]，则中间会构造出一个形状为(test_size, train_size, D)的np矩阵，如果还用np.float64，直接爆内存了，需要57个G；如果改成np.float32，虽然能运行了，但速度很慢，完全没有起到加速的效果。一种思路是把平方差给展开，然后给dists矩阵中的每个元素先加上两个平方和的部分，再减去两倍的交叉乘积的部分。中间主要是用到np.dot，然后也不需要一次性构造出一个特别大的np矩阵，内存不会爆
### 细枝末节：
#### argmax返回的是第一个达到最大值的元素的第一次出现的索引
#### 写no-loop代码的时候，因为dists矩阵的形状是有固定要求的，所以
#### `
####    dists += np.sum(self.X_train ** 2, axis=1)[np.newaxis, :]
####    dists += np.sum(X ** 2, axis=1, keepdims=True)
#### `
#### 两行略有不同
#### np.concatenate作用的参数不局限于np数组，列表/元组/其它序列型的数据，重点是其中要包含待拼接的矩阵；然后里面的每个矩阵除了待拼接的维度外，其它维度都要相等
#### 

# svm部分(这个svm其实不是指支持向量机，而是指用线性分类器的时候，损失函数用的是svm的hinge loss)
## 流程：
### 1.先是对数据的预处理（加载，划分数据集，把图像展平，对数据特征维度进行增广（这样就只需要优化一个权重矩阵W，而不需要单独考虑bias了））；
### 2.完成损失计算和梯度求解的代码。
#### 同样有朴素版本和向量版本两种。朴素版本的话，损失代码它已经给出了，不用自己写（实际也就是按照hinge loss的定义，很顺畅的思路）；梯度的求解的话，主要难点可能在于hinge loss定义里的max，好像不像通常的连续可导函数那样求解起来很方便。其实如果回到求导的定义，再先考察一下最简单的情况，即只有两类，那么，如果max的部分是0，此时0对参数求导是0；如果max的部分是score - correct_class_score + margin，这个部分对W进行求导的话，会发现本质是对W的两列进行求导，一个是当前的类对应的那一列，另一个是correct class对应的那一列。所以，对每一个样本，都先标记一下它在哪些类上面产生了损失，自然就会有梯度（具体可看看代码，感觉说不太清楚）；另外就是别忘了除以num_train，因为计算损失的时候就有这一步求平均；以及，正则化项的梯度也要算上，因为损失里有这一部分（其实从损失函数的组成部分来看就不容易漏了：分类损失+正则化项）
#### 向量版本的话，关于损失函数的编写，正则化项是没有变化的，重点在于前面的分类损失。还是可以先从简单的情况出发，看看一个样本，三个类的时候是怎么个情况。观察到求出所有分数以后，依据hinge loss的定义，它们都需要减去正确类别的分数，并加上1，那这些共性的操作其实就可以向量化。如果能取出每个样本的正确类别的分数，再让它在所有类别上的分数均减去这个正确分数，然后加1，就可以批量判断是否产生了分类损失了；之后就是一些mask和sum的操作了，比较简单了
### 3.完成SGD的代码
#### 先是完善一段train函数的代码，中间会有sgd的实现，很简单；训练就正常训，sgd也只不过是从训练数据里采样了而已。之后是完善一段predict的代码，也很简单，因为这里用的就是一个线性分类器，只不过前面训练的时候损失函数用的是hinge loss。再后面就是一个调参的过程，在validation dataset上调一下lr和reg，没什么技巧，练个手熟（从那个结果来看，lr比reg重要太多了）剩下的一点就是一些可视化的东西，帮助理解。

# softmax classifier部分
## 大体流程和svm完全一样，只有中间的部分操作改变了。
### 1.数据预处理部分完全一样。
### 2关于损失和梯度的求解。同样是分朴素和向量化两种版本。
#### 朴素损失很好求，定义即可。注意的点就是，softmax的时候有一步指数的操作，如果不处理一下，可能会遇到一种情况：一个比较大的数，比如几十几百，再指数，就爆了。因此，对所有的原始probabilities都减去最大值，这样所有的probabilities都是非负数了，就不会爆了。用定义可以很容易推导出这种做法只是一种等价变形，不影响loss值和后续的梯度求解。朴素梯度求解的话，根据每个样本的loss表达式，梯度表达式其实很容易求；又因为朴素写法允许用循环，那么每个循环里进行处理其实是比较容易的。
#### 向量化损失其实也还好，毕竟它这个损失是连续的。将权重矩阵与数据进行点乘，得到原始的probabilities矩阵以后，定义里的归一化和取负对数是很容易的，主要就是取出每个样本的对应类别上的负对数概率这一步。就是一个关于矩阵的索引的小技巧：构造一个样本序号的索引num_idx，从0，1一直到N-1；同时配套的有每个样本的类别的索引，其实就是y；故取出这一步就是scores[num_idx, y]。关于向量化梯度的求解，则相对麻烦一点。如果把每个样本的损失-log($\frac{e^{score_{i}}}{\sum_{j}e^{score_{j}}}$)拆解为$-score_{i} + log(\sum_{j}e^{score_{j}})$，两项分别求梯度，那么向量表达第一项梯度的的关键在于标记，即标记这个样本是属于W的哪一列的梯度，也即用每个样本的标签来指示它属于W的哪一列。为此，可以将标签转换成one-hot向量，然后捣鼓出一个矩阵乘法，发现它刚好可以满足。比如，将所有样本按行排列（其实就是notebook中的排列顺序，X的形状为(N, D)）；然后整理出一个独热标签矩阵label，形状为(C, N)；这样np.dot(label, X)即得到第一部分的梯度。至于第二部分的梯度，会发现每个样本对W的每一列的梯度都有贡献，并且这个“贡献度”是按照归一化后的probabilities来计算的。最后发现需要进行一个广播的操作，将X形状变为(N, D, 1)，将归一化后的概率矩阵形状变为(N, 1, C)，二者进行*操作。这里的重点在于，如果用np.dot()，会报内存不足的错；可能是因为这个方法中间会需要构造一个形状为(N, D, C)的矩阵来存储数据；用 * 操作则没问题
#### 看到一个关于梯度求解的代码，比较简洁，一步到位了
```num_train = X.shape[0]
  scores = X.dot(W)
  scores = scores - np.max(scores, axis=1, keepdims=True)
  
  
  sum_exp_scores = np.exp(scores).sum(axis=1, keepdims=True)
  softmax_matrix = np.exp(scores)/sum_exp_scores  # (N, C)
  loss = np.sum(-np.log(softmax_matrix[np.arange(num_train), y]) )

  softmax_matrix[np.arange(num_train),y] -= 1
  dW = X.T.dot(softmax_matrix)
  ```
### 3.调参。很简单，和svm时候的代码完全一样。

# two_layer_net部分
## 流程：
### 1.数据加载与预处理（这部分一来挺简单，没做什么操作，只是加载进来以后划分了一下数据集，但最主要的是也不需要我们手动实现）
### 2.实现几种层的前向和后向（这里实现前向和后向是搭配起来的，所以前向的时候需要返回cache，存储用于计算梯度的变量；后向则是根据cache和上游传来的梯度进行链式法则，返回到这一层为止的梯度）（然后这里涉及的层主要是affine，relu，svm_loss和softmax_loss。之所以涉及两种损失，是因为当我们把损失计算认为是对最后一层的结果进行操作、计算得到loss时，它也就是一个层，当然可以计算前向后向，而且也需要计算前向后向）
#### 几种层的前向的原理其实就是定义，而且之前都实现过了，只不过这里还需要返回cache，存储一些中间变量；然而计算梯度依赖于哪些变量其实也很一目了然，并且实现的时候注释里也说了要返回哪些变量，所以这一部分没什么问题；关于后向的部分，这一层的结果对于各个变量的梯度也好求，并且之间其实也都实现过了，那之前都进行过梯度下降了，已经写过求解梯度的代码了，之所以不同，是因为之前不管是knn（它压根就没用到梯度下降来优化）还是softmax classifier，svm classifier（这两个本质就是一个线性分类器，只不过用的损失函数不同，但关键在于它们都是一层的，所以没有多个梯度，也不需要应用链式法则，求出来的梯度就是最终的梯度），都不需要应用求导的链式法则，所以前面几个notebook里，前向和后向的代码都很简单，没有什么cache和上游梯度。而正常情况下，网络结构都会比较复杂，则一定需要多层，会用上链式法则。（那其实这里对于矩阵求导的法则，其实都可以不那么熟练，因为这里的层都很简单，仿照一维的情形，再结合维度的分析（即考虑两个梯度怎么乘，才能得到这个变量的形状），就可以求得。这个维度分析dimension analysis也就是cs231n官网上给出的一种方法。不过感觉还是懂严格的求导方法比较好）
#### （依稀记得这里有个坑，因为它要计算数值梯度来验证解析梯度的表达式是否正确，而计算数值梯度的时候会多次调用loss，并且看了它提供的源码，是允许修改数据的，所以如果求解析梯度代码的时候多次对同一个变量操作，可能就会出问题，然后对着代码看了半天，验证了半天，感觉逻辑是对的，不知道问题出在哪。。。一行行调试的时候才稍微看出点情况。。）
#### （中间还实现了一个sandwich layers，就是说一种常见的搭配是affine+relu，二者经常一起用，可以再定义一个新的层，就是两个连着操作。这个挺简单的，毕竟前面都实现过了）
### 3.具体实现一个两层的网络
#### 主要任务就是读懂它给的接口，按照它的要求，实现net的初始化和求梯度，求损失。主要复杂的地方可能在读懂它的接口和后面真正把多层的梯度串起来看，剩下的话，因为各个部分的代码都已经写过了，也比较熟悉了（当然，自己写的代码也需要回看一下，按照对应的要求来）。把梯度串起来那部分，重点在于搞清楚cache存的是什么，求梯度的时候到底用的是哪个cache
### 4.按照Solver类的定义，进行调参，训练
#### 这一部分同样主要复杂在看它的类的接口，看看怎么把超参数传进去，各个函数的作用和返回值是什么。调参的话就正常调（lr真的太重要了，lr设的不好，比如这里，设到1e-2的话，loss很快就炸了）（关于lr，感觉其实可以先大体确定一个量级，再在量级里找最优的）
#### （中间它还提供了帮助我们调参的代码，实现了不同epoch时，loss随iteration的变化，accuracy随epoch的变化，以及第一层权重的可视化。个人感觉还是第二个最实用，其它的更多是用于理解？不过这确实是个辅助调参的办法）