# FullyConnectedNets部分
## 流程：
### 1.完成一个全连接网络的类的实现，要求能实现任意多层（一开始还不需要dropout和bn/ln，仅由affine和relu组成），能反向传播
#### 类的实现主要有两部分，一个是初始化，一个是求loss和grad。初始化的话注意几个细节：任意多层的实现，要区分最后一层和其它层，因为代码写法不一样；存储参数注意按它的要求进行存储；下标从0开始还是从1开始要看清楚。求loss和grad的话，其实所有的层的前向和后向的函数，在assignmeng1中已经实现过了，这里只需要正确调用，或者说想起来要调用，别又想着要自己写一遍，然后感叹工作量过大。。（这里再说一下为什么求梯度的时候要求一个dx，因为它相当于是一个上有梯度。就比如第l层的结果，记为x，传到第l+1层，进行计算。然后反向传播的时候，对第l层参数的梯度，是第l+1层对x的梯度再乘以x对第l层各个参数的梯度。所以每一层都需要求一个dx（虽然第一层的话就不需要dx了，但谁也不能确定这个层到底会出现在哪里）
#### 实现了类以后，调参和分析敏感性之类的相对简单了
### 2.实现几种常见的优化方式（sgd+momentum，RMSProp，Adam），并用它们来进行训练，看看效果如何（但为什么我的效果几乎都反过来了，明明Adam的效果应该最好的，结果反而最差；而且loss确实下降得很好，但accuracy反而比不过其它几个优化器？）（现在知道了，一是之前softmax loss的实现里有个小错误，就是为了避免分子为0的时候，加上的那个“很小的数”ε不够小，导致影响了梯度的精度）
### 3.全力炼丹调参（此时需要加入bn/ln以及dropout了，所以最好先完成BatchNormalization和Dropout两个notebook）。完成了Batchnormalization和Dropout以后，关于网络的修改已经在那两个notebook里完成了，这里只需要完成一下训练的代码。训练之前可以学一下那份参考实现，先在少一点的数据上，跑少几轮，多试试参数，找出里面效果最好的，其实就可以用了；或者找出里面比较好的几组，之后正式训练可以就用这几组参数来训。反正就是尽量避免上来直接开始在自己随意选出的几个值进行组合然后训练，效率不高，很花时间

# BatchNormalization部分
## 流程：
### 1.实现基本的bn的前向反向传播代码。前向很简单，照着定义来；反向的时候需要求三部分，对gamma，对beta和对x的梯度。前两个都好求，就第三个比较难求。可以依据计算图的方法，利用链式求导法则（这真不是说说而已，自己手写推一遍才知道它已经是很高效率了，而且还得静下心来慢慢推导，很容易出错）（或者其实因为这里的公式虽然已经比较复杂了，但也还是可以尝试直接求中间的x_hat对x的导数，只是需要把公式里的mean和std都换成x相关的表达式。推了好几遍了，不知道为啥还是有错，检查了感觉没什么问题啊。。。晚点看看吧）（嗯，总算是用计算图和理论求导写出来了，太麻烦了，错的都是一些细节，而且有些关于链式法则的地方还是有点困惑，到底哪些变量要对另一个变量求偏导？明明x_hat是x，mean，stabel_std三者的函数，但是为什么求x_hat对mean的梯度的时候还要考虑stable_std对mean的梯度？（但这么一写好像也没问题啊。。。）（还有就是，它说在batchnorm_backward_alt中可以通过写出解析的梯度的表达式，做到加速。。但是目前试了一下，如果所谓的解析梯度只是类似于把之前计算图的一步步步骤给整合起来，是做不到加速的。。不知道它还做了什么处理，或者它的所谓解析梯度是如何求的。。）（总算是找出问题了。。。之前在assignment1中实现softmax_loss的时候，取对数的里面为了防止分子为0进而导致对0取对数，给分子加了一个“小数”，但当时只是设置为1e-6；而这个数其实是不够小的，然后在这里就导致了最开始的softmax的dx有一些误差，误差这么叠加着传下去，导致各个参数的梯度都无法达到误差1e-7及以下的量级。。改成1e-20以后就没问题了。。。）

### 2.把bn加到之前的全连接网络里（现在这个全连接网络可以添加bn层了）。加进去也没什么，毕竟bn层的函数已经写好了，接口是类似的，只需要在全连接网络里的loss再加一两行代码就行。后面也演技了一下bn的作用，做了一些对比的实验，发现bn的效果确实好，收敛快，过拟合也没那么严重
### 3.讨论bn与weight scale和batch size之间的关系
#### 关于与weight scale的关系，通过对比实验发现，加入bn后，对weight scale的鲁棒性更强了，在各种不同的weight scale下，加入bn的模型基本都比不加bn的模型的泛化性更好，准确率更高，收敛也更快一点
### 4.实现ln的前向后向传播代码，然后加到网络里。前向很简单，后向的话其实基本可以参照bn，因为如果把它们的计算图都画出来，会发现结构是一样的，主要的区别就是求mean和std的时候，沿着哪个轴进行计算是不太一样的，mean和std的形状也不一样，从而影响了一些代码。但大体是可以参照的
### 5.讨论ln与batch size的关系。同样基本上呈现出了batch size越大，ln效果越好的现象。且开了ln确实会比没开ln要好（但不得不说，中间关于bn，ln效果的探讨，实际并没有太搞清楚，几乎都是直观上感觉的。这一块要严格点的话可能得找论文学一下了，目前的话完全没有什么严格的思路或描述方法

# Dropout部分

# ConvolutionalNetwork部分
## 流程：
### 1.实现卷积和池化的前向后向传播代码（中间有它提供的快速卷积和池化的代码，以及把几种层放到一起实现的sandwich层，不需要自己写，主要看看效果），然后搭一个三层的卷积网络
### 2.实现spatial batchnorm和spatial groupnorm（感觉这个比上面的难多了，主要是高维矩阵的transpose操作没太搞明白含义）（spatial batchnorm通过一定的操作之后，将其转化乘二维的，可以调用之前实现的针对二维输入的batchnorm的代码来实现；spatial groupnorm则还是需要手动实现，它可以看成是instance norm和layer norm的推广，instance norm和layer norm是它的两个极端。看它那个可视化的图就能明白）（但求group norm的梯度还是求的迷迷糊糊的）

# Pytorch部分
## 流程：
### 1.讲解Pytorch中的三个层级，从低级到高级分别是Barebone，module，Sequential，然后分别实现一下（顺带讲一下一些基本模块的功能，比如nn.init，optim）
#### Barebone就是方便一点的numpy版本，用numpy的时候前向和后向的函数都需要我们自己写，调用函数的时候也需要自己把参数传进去；而Barebone版本则是方便在：前向函数可以调用，后向部分pytorch帮忙做了，loss.backward()就可以完成反向传播，然后各个参数的梯度就都得到了，但对于参数的更新还是需要自己去做的，就是自己去给每个参数减去梯度 * lr。
#### module相比Barebone会智能很多。前向的各种函数被打包成module，每个module就可以顺便管理它对应的参数，因此前向的时候不再需要把参数传进去，因为里面就有；反向传播的时候求梯度依然是一句loss.backward，但不需要自己给每个参数进行更新了，因为此时引入了optim，里面打包好了
#### Sequential其实就类似一个复合了多个层的模型，它可以一路按序执行下来。如果有一些比较重复的层，把它们一起放到Sequential里，执行起来可能会方便一点。如果想纯用它搭建模型，可能有一些操作不太适合它这种一路执行到底的方式（比如残差连接），但如果只是正常用module搭建模型的中间，用一下Sequential，简化一些地方，还是挺好的
### 2.自己用各种方法，搭建模型，在cifar-10上争取达到70%+的test acc。只能说调起来没那么简单；以及，结构对了，训练起来就会容易很多，参数调了，也不会过于影响效果